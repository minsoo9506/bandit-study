- Exploitative methods vs Bandit methods

![img](../Images/Explore,%20Exploit,%20and%20Explain:%20Personalizing%20Explainable%20Recommendations%20with%20Bandits/01.png)


# 방법론
- 하고자 하는 것: building a machine learning approach that sorts both explanations and items in a personal, dynamic, responsive, and relevant fashion

## Bart (BAndits for Recsplanations as Treatments)
- standard MAB 는 contextual information 을 고려하지 않는다.
  - contextual information: user features, item features, time of day ...

### Context & reward model
- reward model 을 logistic regression model 로 define 해보자.
  - 편한 설명을 위해 binary reward (ex. click) 를 예시로 사용하지만 당연히 수치형도 가능
- $j$: item, $e$: explanation of item, $x$: context
- $x' = [1_j^T, 1_e^T, x^T], \theta = [\theta_j^T, \theta_e^T, \theta_x^T]$

$$r(j,e,x) = \sigma(\theta_{\text{global}} + \theta^T x')$$

- 위와 같은 경우 linear 하다는 한계가 있으니 interaction 을 고려한 factorization machine 모델을 사용할 수도 있다.
  - latent embedding: $v$

### Off-Policy Training
- contextual bandit 훈련을 위해 CRM (counterfactual risk minimization) 사용
  - importance sample reweighting 사용
  - data 가 production policy $\pi_c$ 에서 생성되었다는 가정

$$\hat{\theta}, \hat{v} = \argmax_{\theta, v}E_{A \sim uniform}[E_{X,R}[\log p_{\theta, v}(R|A,X)]] \\ \approx \argmax_{\theta, v} \frac{1}{N} \sum_{n=1}^N \frac{uniform(a_n)}{\pi_c (a_n)} \log p_{\theta, v}(r_n | a_n,x_n)$$


?? off policy training 빠로 찾아봐야할듯

### Exploration-Exploitation Policy & Propensity Scoring